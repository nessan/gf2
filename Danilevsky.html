<!-- HTML header for doxygen 1.15.0-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
    <head>
        <meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=11" />
        <meta name="generator" content="Doxygen 1.15.0" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <title>GF2++: Danilevsky&#39;s Method</title>
        <link href="tabs.css" rel="stylesheet" type="text/css" />
        <script type="text/javascript" src="jquery.js"></script>
        <script type="text/javascript" src="dynsections.js"></script>
        <script type="text/javascript" src="clipboard.js"></script>
        <link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript" src="cookie.js"></script>
 <link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
 <script type="text/javascript">
window.MathJax = {
  options: {
    ignoreHtmlClass: 'tex2jax_ignore',
    processHtmlClass: 'tex2jax_process'
  },
  loader: {
    load: ['[tex]/mathtools']
  },
  tex: {
    macros: {},
    packages: {
        '[+]': ['mathtools']
    }
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async="async" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-chtml.js"></script>
        <link href="doxygen.css" rel="stylesheet" type="text/css" />
        <link href="doxygen-awesome.css" rel="stylesheet" type="text/css"/>
<link href="doxygen-awesome-sidebar-only.css" rel="stylesheet" type="text/css"/>
<link href="doxygen-awesome-sidebar-only-darkmode-toggle.css" rel="stylesheet" type="text/css"/>
<link href="custom.css" rel="stylesheet" type="text/css"/>
        <!-- ... other metadata & script includes ... -->
        <script type="text/javascript" src="doxygen-awesome-darkmode-toggle.js"></script>
        <script type="text/javascript">
            DoxygenAwesomeDarkModeToggle.init();
        </script>
    </head>
    <body>
            <div id="top">
                <!-- do not remove this div, it is closed by doxygen! -->
                <div id="titlearea">
                    <table cellspacing="0" cellpadding="0">
                        <tbody>
                            <tr id="projectrow">
                                <td id="projectalign">
                                    <div id="projectname">
                                        GF2++
                                    </div>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                <!-- end header part -->
            </div>
        </div>
    </body>
</html>
<!-- Generated by Doxygen 1.15.0 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search/",'.html');
</script>
<script type="text/javascript">
$(function() { codefold.init(); });
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search',true);
  $(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(function(){initNavTree('Danilevsky.html','',''); });
</script>
<div id="container">
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">Danilevsky's Method </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="md_docs_2pages_2Notes_2Danilevsky"></a></p>
<h1 class="doxsection"><a class="anchor" id="introduction-14"></a>
Introduction</h1>
<p><a href="http://web.tecnico.ulisboa.pt/~mcasquilho/compute/com/,eigen/FaddeevaDanil.pdf">Danilevsky's algorithm</a> is a method to compute the coefficients of the <a href="https://en.wikipedia.org/wiki/Characteristic_polynomial">characteristic polynomial</a> for a square matrix.</p>
<p>It isn't well known, so we first review how it works for real-valued matrices, and then show how it can be adapted to work for bit-matrices.</p>
<p>Bit-matrices are matrices over <a href="https://en.wikipedia.org/wiki/GF(2)">GF2</a>, the simplest <a href="https://en.wikipedia.org/wiki/Galois_field">Galois field</a> with just two elements 0 &amp; 1 where addition/subtraction and multiplication/division operations are all done mod two which keeps everything closed in the set \(\{0,1\}\).</p>
<h1 class="doxsection"><a class="anchor" id="characteristic-polynomials-1"></a>
Characteristic Polynomials</h1>
<p>If \(A\) is an \(n \times n\) matrix, the eigenvalues and eigenvectors of \(A\) satisfies the equation</p>
<p>$$ A \cdot v = \lambda v. $$</p>
<p>Solving that equation is equivalent to finding solutions for the linear system</p>
<p>$$ (A - \lambda I) v = 0. $$</p>
<p>which has solutions if and only if \(A - \lambda I\) is singular so</p>
<p>$$ |A - \lambda I| = 0 $$</p>
<p>where we are using \(|\cdot|\) to denote the determinant.</p>
<p>The <a href="https://en.wikipedia.org/wiki/Characteristic_polynomial">characteristic polynomial</a> of \(A\) is defined by the determinant \(|A - \lambda I|\) thought of as a function of \(\lambda\):</p>
<p>$$ c(\lambda) = |A - \lambda I|. $$</p>
<p>Expanding the determinant, you can explicitly get a few of the terms in this polynomial as</p>
<p>$$ c(\lambda) = (-\lambda)^n + tr(A)(-\lambda)^{n-1} + \cdots + \det(A) $$</p>
<p>where \(tr(A)\) is the trace of the matrix \(A\). However, it is not practical to compute all the terms in the polynomial by brute force expansion like this.</p>
<p>Even if we have all the coefficients in the characteristic polynomial, getting the eigenvalues means solving</p>
<p>$$ c(\lambda) = 0. $$</p>
<p>However, extracting the roots of a high-order polynomial is very difficult. For this reason, that method is only practical for the small matrices that turn up in homework exercises!</p>
<p>The <a href="https://en.wikipedia.org/wiki/Characteristic_polynomial">characteristic polynomial</a> is still important even if we cannot readily get the eigenvalues from it. For one thing, the well-known <a href="https://en.wikipedia.org/wiki/Cayley%E2%80%93Hamilton_theorem">Cayley-Hamilton</a> theorem tells us that the characteristic polynomial is an <a href="https://en.wikipedia.org/wiki/Annihilating_polynomial">annihilating polynomial</a> for the matrix, which means that</p>
<p>$$ c(A) = 0. $$</p>
<p>This result is helpful in various applications. For this reason, we are still interested in computing the coefficients \(c_i\) of the characteristic polynomial written as</p>
<p>$$ c(\lambda) = \lambda^n + c_{n-1} \lambda^{n-1} + \cdots + c_1 \lambda + c_0. $$</p>
<p>(where without loss of generality, we take \(c_n = 1\))</p>
<h1 class="doxsection"><a class="anchor" id="companion-matrices"></a>
Companion Matrices</h1>
<p>Investigating square matrices of size \(n\) led us to consider polynomials of order \(n\).</p>
<p>How about the reverse? If you have an arbitrary polynomial of the form</p>
<p>$$ c(\lambda) = \lambda^n + c_{n-1} \lambda^{n-1} + \cdots + c_1 \lambda + c_0. $$</p>
<p>is there a matrix with that as its characteristic polynomial?</p>
<p>Here is one we will show works:</p>
<p>$$ C = \begin{bmatrix} -c_{n-1} &amp; -c_{n-2} &amp; -c_{n-3} &amp; \ldots &amp; &amp; -c_2 &amp; -c_1 &amp; -c_0 \\ 1 &amp; 0 &amp; 0 &amp; \ldots &amp; &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; \ldots &amp; &amp; 0 &amp; 0 &amp; 0 \\ &amp; &amp; &amp; \ldots \\ 0 &amp; 0 &amp; 0 &amp; \ldots &amp; &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; \ldots &amp; &amp; 0 &amp; 1 &amp; 0 \end{bmatrix} $$</p>
<p>This matrix \(C\) has ones on the sub-diagonal and the polynomial coefficients (with a minus sign) along the first row. It is an upper <a href="https://en.wikipedia.org/wiki/Hessenberg_matrix">Hessenberg</a> matrix.</p>
<p>Computing the determinant is difficult for all but the smallest <em>general</em> matrices. However, getting the determinant for <em>triangular</em> matrices is trivial, as you only need to multiply the elements on the diagonal. <a href="https://en.wikipedia.org/wiki/Hessenberg_matrix">Hessenberg</a> matrices are <em>almost</em> triangular and also quite amenable when it comes to computing determinants.</p>
<p>To see that our \(C\) has the characteristic polynomial, we want to consider the determinant:</p>
<p>$$ |\lambda I - C| = \begin{vmatrix} \lambda+c_{n-1} &amp; c_{n-2} &amp; c_{n-3} &amp; \ldots &amp; &amp; c_2 &amp; c_1 &amp; c_0 \\ 1 &amp; \lambda &amp; 0 &amp; \ldots &amp; &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; \lambda &amp; \ldots &amp; &amp; 0 &amp; 0 &amp; 0 \\ &amp; &amp; &amp; \ldots \\ 0 &amp; 0 &amp; 0 &amp; \ldots &amp; &amp; 1 &amp; \lambda &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; \ldots &amp; &amp; 0 &amp; 1 &amp; \lambda \end{vmatrix} $$</p>
<p>When you expand that determinant by the first row, then you get</p>
<p>$$ (\lambda + c_{n-1}) \begin{vmatrix} &amp; \lambda &amp; 0 &amp; \ldots &amp; &amp; 0 &amp; 0 &amp; 0 \\ &amp; 1 &amp; \lambda &amp; \ldots &amp; &amp; 0 &amp; 0 &amp; 0 \\ &amp; &amp; &amp; \ldots \\ &amp; 0 &amp; 0 &amp; \ldots &amp; &amp; 1 &amp; \lambda &amp; 0 \\ &amp; 0 &amp; 0 &amp; \ldots &amp; &amp; 0 &amp; 1 &amp; \lambda \end{vmatrix} + c_{n-2} \begin{vmatrix} &amp; 1 &amp; 0 &amp; \ldots &amp; &amp; 0 &amp; 0 &amp; 0 \\ &amp; 0 &amp; \lambda &amp; \ldots &amp; &amp; 0 &amp; 0 &amp; 0 \\ &amp; &amp; &amp; \ldots \\ &amp; 0 &amp; 0 &amp; \ldots &amp; &amp; 1 &amp; \lambda &amp; 0 \\ &amp; 0 &amp; 0 &amp; \ldots &amp; &amp; 0 &amp; 1 &amp; \lambda \end{vmatrix} + \dots + c_0 \begin{vmatrix} &amp; 1 &amp; 0 &amp; \ldots &amp; &amp; 0 &amp; 0 &amp; 0 \\ &amp; 0 &amp; 1 &amp; \ldots &amp; &amp; 0 &amp; 0 &amp; 0 \\ &amp; &amp; &amp; \ldots \\ &amp; 0 &amp; 0 &amp; \ldots &amp; &amp; &amp; 1 &amp; \lambda \\ &amp; 0 &amp; 0 &amp; \ldots &amp; &amp; 0 &amp; 0 &amp; 1 \end{vmatrix} $$</p>
<p>The \((n-1) \times (n-1)\) <a href="https://en.wikipedia.org/wiki/Minor_(linear_algebra)">co-factors</a> are all triangular determinants that are readily computed and we get</p>
<p>$$ |\lambda I - C| = (\lambda + c_{n-1}) \lambda^{n-1} + c_{n-2} \lambda^{n-2} + \cdots + c_0. $$</p>
<p>That is exactly the form we want.</p>
<dl class="section note"><dt>Note</dt><dd>You can make the same determinant expansion argument if the polynomial coefficients were in the final column instead of the top row. That is the version seen in some expositions.</dd></dl>
<p>Here \(C\) is a "companion" for the polynomial \(c(\lambda)\) and is known as a <a href="https://en.wikipedia.org/wiki/Companion_matrix">companion matrix</a>.</p>
<h1 class="doxsection"><a class="anchor" id="frobenius-form"></a>
Frobenius Form</h1>
<p>For any polynomial \(c(\lambda)\) we can readily construct a companion matrix \(C\) that has \(c(\lambda)\) as its characteristic polynomial.</p>
<p>Now <a href="https://en.wikipedia.org/wiki/Matrix_similarity">similar matrices</a> all have the same characteristic polynomial, so we can generate many matrices with the same characteristic polynomial by similarity transformations of \(C\). In particular, if \(M\) is any invertible \(n \times n\) matrix, then \(M^{-1} \cdot C \cdot M\) will have the same characteristic polynomial as \(C\).</p>
<p>Now let \(A\) be an arbitrary \(n \times n\) matrix.</p>
<p>A natural question then is whether you can find some invertible \(M\) such that \(C = M^{-1} \cdot A \cdot M\) is in <a href="https://en.wikipedia.org/wiki/Companion_matrix">companion matrix</a> form. If so, you can read the characteristic polynomial coefficients for \(A\) off the top row of \(C\).</p>
<p>This isn't possible in general, but you can always find a similarity transformation that converts \(A\) to <a href="https://encyclopediaofmath.org/wiki/Frobenius_matrix">Frobenius form</a>. A Frobenius form matrix is block-diagonal, where each block is a <a href="https://en.wikipedia.org/wiki/Companion_matrix">companion matrix</a>.</p>
<p>So there is always some \(M\) such that</p>
<p>$$ A = M^{-1} \cdot F \cdot M. $$</p>
<p>where the matrix \(F\) has the block-diagonal form</p>
<p>$$ F = \begin{bmatrix} C_0 &amp; 0 &amp; 0 &amp; \ldots &amp; 0 &amp; 0 \\ 0 &amp; C_1 &amp; 0 &amp; \ldots &amp; 0 &amp; 0 \\ &amp; &amp; &amp; \ldots \\ 0 &amp; 0 &amp; 0 &amp; \ldots &amp; 0 &amp; C_{k-1} \end{bmatrix} $$</p>
<p>and each of the \(k\) diagonal blocks is a <a href="https://en.wikipedia.org/wiki/Companion_matrix">companion matrix</a>.</p>
<p>Each companion matrix block has a trivially computable characteristic polynomial: \(c_i(\lambda)\) for \(i = 0, \ldots, k-1\). The characteristic polynomial of \(F\) and hence by similarity, \(A\) is just:</p>
<p>$$ c(\lambda) = \prod_{i = 0}^{k-1} c_i(\lambda). $$</p>
<blockquote class="doxtable">
<p>If \(A\) is an arbitrary real-valued \(n \times n\) matrix, <a href="http://web.tecnico.ulisboa.pt/~mcasquilho/compute/com/,eigen/FaddeevaDanil.pdf">Danilevsky's algorithm</a> applies a sequence of similarity transformations that, step by step, efficiently moves it to <a href="https://encyclopediaofmath.org/wiki/Frobenius_matrix">Frobenius form</a>. </p>
</blockquote>
<h1 class="doxsection"><a class="anchor" id="danilevsky-for-real-matrices"></a>
Danilevsky for Real Matrices</h1>
<p>We will describe how the algorithm works for an \(n \times n\) matrix \(A\) with elements in \(\mathbb{R}\). The algorithm can be written as</p>
<p>$$ A = A_1 \rightarrow A_2 \rightarrow \ldots \rightarrow A_n $$</p>
<p>where that final matrix, \(A_n\), is in Frobenius form. At each stage, \(A_{k+1}\) is constructed from its predecessor \(A_{k}\) via a similarity transformation:</p>
<p>$$ A_{k+1} = M^{-1}_{k} \cdot A_k \cdot M_k. $$</p>
<p>Here "\f$\cdot\f$" denotes matrix multiplication.</p>
<p>The algorithm is efficient because we can readily construct the \(M_k\)'s and their inverses. Moreover, they are <em>sparse</em> matrices, meaning those two matrix multiplications can be performed quickly in \(O(n^2)\) operations instead of the usual \(O(n^3)\).</p>
<p>Starting with a general matrix such as</p>
<p>$$ A := A_1 = \begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} &amp; \ldots &amp; a_{1n-2} &amp; a_{1n-1} &amp; a_{1n} \\ a_{21} &amp; a_{22} &amp; a_{23} &amp; \ldots &amp; a_{2n-2} &amp; a_{2n-1} &amp; a_{2n} \\ &amp; &amp; &amp; \ldots \\ &amp; &amp; &amp; \ldots \\ a_{n-11} &amp; a_{n-12} &amp; a_{n-13} &amp; \ldots &amp; a_{n-1n-2} &amp; a_{n-1n-1} &amp; a_{n-1n} \\ a_{n1} &amp; a_{n2} &amp; a_{n3} &amp; \ldots &amp; a_{nn-2} &amp; a_{nn-1} &amp; a_{nn} \end{bmatrix} $$</p>
<p>we would like to get to the companion matrix form</p>
<p>$$ A_n = \begin{bmatrix} c_1 &amp; c_2 &amp; c_3 &amp; \ldots &amp; &amp; c_{n-2} &amp; c_{n-1} &amp; c_{n} \\ 1 &amp; 0 &amp; 0 &amp; \ldots &amp; &amp; 0 &amp; 0 &amp; 0 \\ &amp; &amp; &amp; \ldots \\ &amp; &amp; &amp; \ldots \\ 0 &amp; 0 &amp; 0 &amp; \ldots &amp; &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; \ldots &amp; &amp; 0 &amp; 1 &amp; 0 \\ \end{bmatrix} $$</p>
<p>The characteristic polynomial for \(A\) is then</p>
<p>$$ c(\lambda) = 1 + c_1 \lambda + c_2 \lambda^2 + \cdots + c_{n-1} \lambda^{n-1} + c_{n} \lambda^{n}. $$</p>
<p>To get things rolling, we need to construct a matrix \(M\) such that \(A \cdot M\) is one step closer to companion matrix form (we are dropping the subscripts on the matrices &ndash; in a computer implementation the matrices mostly just get updated in-place anyway).</p>
<p>Our aim is to find \(M\) so that \(B := A \cdot M\) has the form</p>
<p>$$ \begin{bmatrix} b_{11} &amp; b_{12} &amp; b_{13} &amp; \ldots &amp; b_{1n-2} &amp; b_{1n-1} &amp; b_{1n} \\ b_{21} &amp; b_{22} &amp; b_{23} &amp; \ldots &amp; b_{2n-2} &amp; b_{2n-1} &amp; b_{2n} \\ &amp; &amp; &amp; \ldots \\ &amp; &amp; &amp; \ldots \\ b_{n-11} &amp; b_{n-12} &amp; b_{n-13} &amp; \ldots &amp; b_{n-1n-2} &amp; b_{n-1n-1} &amp; b_{n-1n} \\ 0 &amp; 0 &amp; 0 &amp; \ldots &amp; 0 &amp; 1 &amp; 0 \end{bmatrix} $$</p>
<p>Assuming that \(a_{nn-1} \neq 0\), an appropriate \(M\) is</p>
<p>$$ M = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; \ldots &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; \ldots &amp; 0 &amp; 0 &amp; 0 \\ &amp; &amp; &amp; \ldots \\ &amp; &amp; &amp; \ldots \\ \frac{-a_{n1}}{a_{nn-1}} &amp; \frac{-a_{n2}}{a_{nn-1}} &amp; \frac{-a_{n3}}{a_{nn-1}} &amp; \ldots &amp; \frac{-a_{nn-2}}{a_{nn-1}} &amp; \frac{1}{a_{nn-1}} &amp; \frac{-a_{nn}}{a_{nn-1}} \\ 0 &amp; 0 &amp; 0 &amp; \ldots &amp; 0 &amp; 0 &amp; 1 \end{bmatrix} $$</p>
<p>That is, \(M\) is simply the identity matrix with the \((n-1)\)'st row replaced as shown. With a bit of staring, you should be able to convince yourself that \(B = A \cdot M\) will indeed have a final row in Frobenius form.</p>
<p>Note that each column \(l\) in \(M\) has at most two non-zero elements namely \(M_{ll}\) and \(M_{n-1l}\) This means that the elements of \(B\) are computed efficiently by considering just a couple of terms instead of the usual \(n\).</p>
<p>$$ b_{ij} = \sum_{l=1}^{n} a_{il}m_{lj} = a_{ij}m_{jj} + a_{in-1}m_{n-1j} $$</p>
<p>So</p>
<p>$$ b_{ij} = a_{ij} - a_{in-1} \frac{a_{nj}}{a_{nn-1}} \text{ for columns } j \ne n-1, $$</p>
<p>and</p>
<p>$$ b_{in-1} = \frac{a_{in-1}}{a_{nn-1}} \text{ for column } n-1. $$</p>
<p>Note that as promised \(b_{nj} = 0 \text{ if} j \ne n-1\) and that \(b_{nn-1} = 1\).</p>
<p>Of course, \(B = A \cdot M\) is <em>not</em> similar the original matrix \(A\) but \(M^{-1} \cdot A \cdot M\) is.</p>
<p>Fortunately \(M^{-1}\) is also readily computed</p>
<p>$$ M^{-1} = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; \ldots &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; \ldots &amp; 0 &amp; 0 &amp; 0 \\ &amp; &amp; &amp; \ldots \\ &amp; &amp; &amp; \ldots \\ a_{n1} &amp; a_{n2} &amp; a_{n3} &amp; \ldots &amp; a_{nn-2} &amp; a_{nn-1} &amp; a_{nn} \\ 0 &amp; 0 &amp; 0 &amp; \ldots &amp; 0 &amp; 0 &amp; 1 \end{bmatrix} $$</p>
<p>From the form of \(M^{-1}\), it is clear that the \(n\)'th row of \(C := M^{-1} \cdot B\) is just the same as the \(n\)'th row of \(B\), so it will be in Frobenius form.</p>
<p>Moreover, once again, \(M^{-1}\) has at most two non-zero terms in each column. Therefore, the matrix product can also be computed very efficiently:</p>
<p>$$ c_{ij} = \sum_{l=1}^{n} m_{il} b_{lj} = m_{ii} b_{ij} = b_{ij} \text{ if } i \ne n-1 $$</p>
<p>while</p>
<p>$$ c_{n-1j} = \sum_{l=1}^{n} m_{n-1l} b_{lj} = \sum_{l=1}^{n} a_{nl} b_{lj} $$</p>
<p>Thus \(C = M^{-1} \cdot A \cdot M\) is similar to \(A\) but one step closer to companion matrix form, <em>and</em> happily, the elements of \(C\) can be efficiently computed using just \(O(n^2)\) operations. If everything goes well, we can repeat these operations for \(n\) steps to arrive at the required Frobenius form shown above. We hit a snag if \(a_{nn-1} = 0\), but we will deal with that below.</p>
<h2 class="doxsection"><a class="anchor" id="algorithm-for-fmathbbrf"></a>
Algorithm for \(\mathbb{R}\)</h2>
<div class="pseudocode"><ol type="1">
<li>Set \(k = n\) and let \(A\) be the input matrix.</li>
<li><b>if</b> \(A_{kk-1} = 0\)<ol type="a">
<li>Look for \(j &lt; k-1\) where \(A_{kj} \ne 0\).</li>
<li>On success, swap rows <em>and</em> columns \(j\) and \(k-1\) of \(A\). <span class="comment">a similarity transformation</span></li>
</ol>
</li>
<li><b>if</b> now \(A_{kk-1} \ne 0\)<ol type="a">
<li>Let \(m = \text{row}_k(A)\) (so \(m_{k-1} = A_{kk-1} \ne 0\)).</li>
<li>For \(i = 1, \ldots, k-1\) and \(j = 1, \ldots, n\) compute $$ B_{ij} = \begin{cases} A_{ij} - A_{ik-1}\frac{m_j}{m_{k-1}} &amp; \text{ if } j \ne k-1 \\ \frac{A_{ik-1}}{m_{k-1}} &amp; \text{ if } j = k-1 \end{cases} $$</li>
</ol>
<ol type="a">
<li>For \(i = 1, \ldots, k\) and \(j = 1, \ldots, n\) update \(A\) $$ \begin{aligned} A_{ij} &amp;= B_{ij} \text{ for } i = 1, \ldots, k-2, \\ A_{k-1j} &amp;= \sum_{l=1}^{n} m_l B_{lj} = m \cdot \text{col}_j(B) \\ A_{kj} &amp;= \begin{cases} 0 &amp; \text { for } j \ne k-1 \\ 1 &amp; \text { for } j = k-1 \end{cases} \end{aligned} $$ Row \(k\) of \(A\) is now in companion matrix form. <span class="comment">the later rows of \(A\) are already there</span></li>
<li>If \(k &gt; 1\), then \(k \leftarrow k-1\), <b>goto</b> step 2, <b>else stop</b>.</li>
</ol>
</li>
<li><b>else</b><ol type="a">
<li>Extract the characteristic polynomial for the lower right block (which is already in companion form).</li>
<li>Recurse using the upper left block as the smaller input matrix.</li>
<li>Convolve the two sets of coefficients to get the final characteristic polynomial. </li>
</ol>
</li>
</ol>
</div><p>If \(A_{kk-1} = 0\) even after trying the search in step 2 then we cannot perform step 3 as that would involve division by zero. However, in that case, the current \(A\) matrix already has a block diagonal form and we can simply extract the characteristic polynomial for the lower right block (which is already in companion form) and recurse using the upper left block as the smaller input matrix.</p>
<p>The current \(A\) matrix must the form:</p>
<p>$$ A = \begin{bmatrix} a_{11} &amp; a_{12} &amp; \ldots &amp; a_{1k-1} &amp; a_{1k} &amp; a_{1k+1} &amp; \ldots &amp; a_{1n-1} &amp; a_{1n} \\ a_{21} &amp; a_{22} &amp; \ldots &amp; a_{2k-1} &amp; a_{2k} &amp; a_{2k+1} &amp; \ldots &amp; a_{2n-1} &amp; a_{2n} \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ 0 &amp; 0 &amp; \ldots &amp; 0 &amp; a_{kk} &amp; a_{kk+1} &amp; \ldots &amp; a_{kn-1} &amp; a_{kn} \\ 0 &amp; 0 &amp; \ldots &amp; 0 &amp; 1 &amp; 0 &amp; \ldots &amp; 0 &amp; 0 \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ 0 &amp; 0 &amp; \ldots &amp; 0 &amp; 0 &amp; 0 &amp; \ldots &amp; 1 &amp; 0 \end{bmatrix} := \begin{bmatrix} A_1 &amp; A_2 \\ 0 &amp; A_3 \end{bmatrix} $$</p>
<p>So if \(I_n\) represents the \(n \times n\) identity matrix then:</p>
<p>$$ \det(\lambda I_n - A) = \det(\lambda I_{k-1} - A_1) \det(\lambda I_{n-k+1} - A_3). $$</p>
<p>Hence the characteristic polynomial \(c_A(x)\) we are after is the product of two other characteristic polynomials:</p>
<p>$$ c_A(x) = c_{A_1}(x) c_{A_3}(x) $$</p>
<p>and as \(A_3\) is already in companion form we can easily read off the coefficients for \(c_{A_3}(x)\)</p>
<p>$$ c_{A_3}(x) = 1 + a_{kk} x + a_{kk+1} x^2 + \ldots + a_{kn} x^{n-k+1}. $$</p>
<p>The algorithm just stores those coefficients and recurses using \(A_1\) as the smaller \((k-1) \times (k-1)\) input matrix. It can then convolve the coefficients of \(c_{A_1}(x)\) and \(c_{A_3}(x)\) to return the coefficients for \(c_{A}(x)\).</p>
<p>[!TIP]</p>
<blockquote class="doxtable">
<p>In the case of real valued matrices it can be that \(a_{kk-1}\) is non-zero but still small. Division by very small floating point numbers should be avoided as those operations tend to be <a href="https://en.wikipedia.org/wiki/Floating-point_arithmetic">ill-conditioned</a>. For that reason, step 2 will always be performed to find the \(a_{kj}\) for \(j &lt; k-1\) that is largest in absolute value and then do the suggested row and column swaps to move that element into the all important \((k,k-1)\) slot. </p>
</blockquote>
<h1 class="doxsection"><a class="anchor" id="danilevsky-for-bit-matrices"></a>
Danilevsky for Bit-Matrices</h1>
<p>In the case of bit-matrices, or in more formal math-speak, matrices with elements in \(\mathbb{F}_2\), the input matrix \(A\) is all zeros or ones. Moreover, the usual addition and multiplication operators are performed modulo 2 so everything remains in the limited set \(\{0,1\}\). In \(\mathbb{F_2}\) we can replace addition with the logical XOR operator and multiplication with the logical AND operator.</p>
<p>Note that in \(\mathbb{F_2}\) we have \(1+1 = 2 \rightarrow 0\) so the additive inverse of 1 is 1. And of course as usual, the additive inverse of 0 is 0. This means that in \(\mathbb{F_2}\) negation is a no-op and any term like \(-b\) can just be replaced with \(b\).</p>
<p>We always have \(1 * 1 = 1\) so the multiplicative inverse of \(1\) is just \(1\). Also just like \(\mathbb{R}\), the element \(0\) has no multiplicative inverse in \(\mathbb{F_2}\) either &ndash; you still cannot divide by zero. This means that if \(a, b \in \mathbb{F_2}\) then a term like \(a/b\) makes no sense if \(b=0\) but otherwise \(a/b = a\).</p>
<p>Let's reconsider that very first step we took above to move our matrix \(A\) closer to Frobenius form. That involved finding a matrix \(M\) such that \(B = A \cdot M\) had its final row in the required format.</p>
<p>Taking into account that now all the matrices are boolean and assuming that \(A_{nn-1} = 1\) then the appropriate \(M\) is:</p>
<p>$$ M = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; \ldots &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; \ldots &amp; 0 &amp; 0 &amp; 0 \\ &amp; &amp; &amp; \ldots \\ &amp; &amp; &amp; \ldots \\ a_{n1} &amp; a_{n2} &amp; a_{n3} &amp; \ldots &amp; a_{nn-2} &amp; a_{nn-1} &amp; a_{nn} \\ 0 &amp; 0 &amp; 0 &amp; \ldots &amp; 0 &amp; 0 &amp; 1 \end{bmatrix} $$</p>
<p>That is, \(M\) is just the identity matrix with the \((n-1)\)'st row replaced with that row from \(A\).</p>
<p>Moreover, using the fact that for any \(x \in \mathbb{F}_2\) we always have \(x + x = 2x = 0\), it is is easy to verify that \(M^{-1} = M\).</p>
<p>So \(C := M \cdot A \cdot M\) will be <em>similar</em> to \(A\) and one step closer to Frobenius form and, because \(M\) is simple and sparse, those two matrix multiplications can be performed very efficiently in \(O(n^2)\) operations.</p>
<p>The full algorithm for matrices over \(\mathbb{R}\) also works for matrices over \(\mathbb{F_2}\). As before, it proceeds in a sequence of steps that move \(A\) closer to companion/Frobenius matrix form at the end of each one.</p>
<h2 class="doxsection"><a class="anchor" id="algorithm-for-fmathbbf_2f"></a>
Algorithm for \(\mathbb{F}_2\)</h2>
<div class="pseudocode"><ol type="1">
<li>Set \(k = n\) and let \(A\) be the input matrix.</li>
<li><b>if</b> \(A_{kk-1} = 0\)<ol type="a">
<li>Look for \(j &lt; k-1\) where \(A_{kj} = 1\).</li>
<li>On success, swap rows <em>and</em> columns \(j\) and \(k-1\) of \(A\). <span class="comment">a similarity transformation</span></li>
</ol>
</li>
<li><b>if</b> now \(A_{kk-1} = 1\)<ol type="a">
<li>Let \(m = \text{row}_k(A)\) (so \(m_{k-1} = A_{kk-1} = 1\)).</li>
<li>For \(i = 1, \ldots, k-1\) and \(j = 1, \ldots, n\) compute $$ B_{ij} = \begin{cases} A_{ij} - A_{ik-1}\frac{m_j}{m_{k-1}} = A_{ij} + A_{ik-1} m_j &amp; \text{ if } j \ne k-1 \\ \frac{A_{ik-1}}{m_{k-1}} = A_{ik-1} &amp; \text{ if } j = k-1 \end{cases} $$</li>
<li>For \(i = 1, \ldots, k\) and \(j = 1, \ldots, n\) update \(A\) $$ \begin{aligned} A_{ij} &amp;= B_{ij} \text{ for } i = 1, \ldots, k-2, \\ A_{k-1j} &amp;= \sum_{l=1}^{n} m_l B_{lj} = m \cdot \text{col}_j(B) \\ A_{kj} &amp;= \begin{cases} 0 &amp; \text { for } j \ne k-1 \\ 1 &amp; \text { for } j = k-1 \end{cases} \end{aligned} $$ Row \(k\) of \(A\) is now in companion matrix form. <span class="comment">the later rows of \(A\) are already there</span></li>
<li>If \(k &gt; 1\), then \(k \leftarrow k-1\), <b>goto</b> step 2, <b>else stop</b>.</li>
</ol>
</li>
<li><b>else</b><ol type="a">
<li>Extract the characteristic polynomial for the lower right block (which is already in companion form).</li>
<li>Recurse using the upper left block as the smaller input matrix.</li>
<li>Convolve the two sets of coefficients to get the final characteristic polynomial coefficients. </li>
</ol>
</li>
</ol>
</div><p>As with the real-valued case, if \(A_{kk-1} = 0\) even after trying the search in step 2 then we cannot perform step 3 as that would involve division by zero. However, in that case, the current \(A\) matrix already has a block diagonal form and we can simply extract the characteristic polynomial for the lower right block (which is already in companion form) and recurse using the upper left block as the smaller input matrix.</p>
<p>The current \(A\) matrix must have the following form:</p>
<p>$$ A = \begin{bmatrix} A_1 &amp; A_2 \\ 0 &amp; A_3 \end{bmatrix} $$</p>
<p>hence the characteristic polynomial \(c_A(x)\) is the product of two others</p>
<p>$$ c_A(x) = c_{A_1}(x) c_{A_3}(x). $$</p>
<p>As \(A_3\) is already in Frobenius form we can easily read off the coefficients for \(c_{A_3}(x)\)</p>
<p>$$ c_{A_3}(x) = 1+ a_{kk} x + a_{kk+1} x^2 + \ldots + a_{kn} x^{n-k+1}. $$</p>
<p>Store those coefficients and recurse using \(A_1\) as the \((k-1) \times (k-1)\) input matrix. Convolve the coefficients of \(c_{A_1}(x)\) and \(c_{A_3}(x)\) to get the coefficients for \(c_{A}(x)\).</p>
<dl class="section note"><dt>Note</dt><dd>In \(\mathbb{F}_2\) any matrix element can only be \(0\) or \(1\). All things being equal, you'd expect to have to perform the recursive fourth step half the time. </dd></dl>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<div id="page-nav" class="page-nav-panel">
<div id="page-nav-resize-handle"></div>
<div id="page-nav-tree">
<div id="page-nav-contents">
</div><!-- page-nav-contents -->
</div><!-- page-nav-tree -->
</div><!-- page-nav -->
</div><!-- container -->
<!-- HTML footer for doxygen 1.15.0-->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a href="md_docs_2pages_2Notes_2Introduction.html">Technical Notes</a></li>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.15.0 </li>
  </ul>
</div>
</body>
</html>
